<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

    <title>HyunJin Kim</title>

    <link rel="stylesheet" href="/assets/styles/github-light.css">
    <link rel="stylesheet" href="/assets/styles/personal.css">
</head>

<body>
    <div class="container">
        <header>
            <h1>HyunJin Kim</h1>
            <img src="/assets/images/members/HJK.jpg" style="width:190px;"><br />
            <p>Ph.D. Candidate
Department of Artificial Intelligence (인공지능학과)
<a href="https://hli.skku.edu">HLI Lab</a>, <a href="https://www.skku.edu/">SKKU</a></p>
<p><a href="mailto:khyunjin1993@g.skku.edu">Email</a> / <a href="https://scholar.google.com/citations?user=gO-nMlsAAAAJ&amp;hl=en">Google Scholar</a> / <a href="https://khyunjin1993.medium.com/">Medium</a> / <a href="https://github.com/agwaBom">GitHub</a></p>
        </header>

        <section class="content">
            
            <h2>About Me</h2>
<p>Hello! I am HyunJin Kim (김현진), a Ph.D. candidate at Sungkyunkwan University (SKKU, 성균관대학교), where I am advised by <a href="https://nosyu.kr/">Prof. JinYeong Bak</a> in the <a href="https://hli.skku.edu/">Human Language Intelligence Lab</a>.</p>
<p>I am also in a Ph.D. Collaborator program with Microsoft, where I have worked under supervision of <a href="https://www.microsoft.com/en-us/research/people/youki/">Dr. Young Jin Kim</a> to explore effective offsite-tuning techniques for generative models.</p>
<p>Currently, I am a research intern at Microsoft Research Asia (MSRA) under the guidance of <a href="https://www.microsoft.com/en-us/research/people/xiaoyuanyi/">Dr. Xiaoyuan Yi</a>. My work at MSRA focuses on superalignment. I was honored with the <strong>Stars of Tomorrow Excellence Award</strong> during internship at MSRA.</p>
<p>My research interests lie in analyzing and developing deep learning model architectures (e.g., Transformers, Mixture-of-Experts) and training methods (e.g., Direct Preference Optimization, Instruction Tuning, etc.).</p>
<p>This has led to the development of task-specific architectures such as <a href="https://dl.acm.org/doi/abs/10.1145/3579856.3582823">AsmDepictor</a>
 and training method like the offsight-tuning, <a href="https://aclanthology.org/2024.naacl-long.336/">PEMA</a>. Currently, my work focuses on advancing scalable oversight methods to achieve superalignment. For detail, please refer our <a href="https://www.arxiv.org/abs/2412.16468">survey</a> and <a href="https://www.arxiv.org/abs/2503.07660">position</a>.</p>
            
            <h2>Education</h2>
<ul>
<li><strong>Sungkyunkwan University</strong>, South Korea.<br />
  Ph.D., Artificial Intelligence, 2023~Present</li>
<li><strong>Sungkyunkwan University</strong>, South Korea.<br />
  M.S., Artificial Intelligence, 2021~2023</li>
<li><strong>Kyonggi University</strong>, South Korea.<br />
  B.S., Computer Engineering (Transferred), 2019~2021<br />
  B.S., Early Childhood Education, 2015~2019</li>
</ul>
            
            <h2>Preprints and Surveys</h2>
<ol>
<li>
<p><strong>Research on Superalignment Should Advance Now with Parallel Optimization of Competence and Conformity</strong><br />
<strong>HyunJin Kim</strong>, Xiaoyuan Yi, Jing Yao, Muhua Huang, JinYeong Bak, James Evans, Xing Xie<br />
  [Axriv] 2025.<br />
<a href="https://www.arxiv.org/abs/2503.07660">PDF</a></p>
</li>
<li>
<p><strong>The Road to Artificial SuperIntelligence: A Comprehensive Survey of Superalignment</strong><br />
<strong>HyunJin Kim</strong>, Xiaoyuan Yi, Jing Yao, Jianxun Lian, Muhua Huang, Shitong Duan, JinYeong Bak, Xing Xie<br />
  [Axriv] 2024.<br />
<a href="https://www.arxiv.org/abs/2412.16468">PDF</a></p>
</li>
</ol>
<h2>International Conferences and Journals</h2>
<ol>
<li>
<p><strong>PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models</strong><br />
<strong>HyunJin Kim</strong>, Young Jin Kim, JinYeong Bak<br />
  [Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)] 2024.<br />
<a href="https://aclanthology.org/2024.naacl-long.336/">PDF</a>
  <a href="https://github.com/agwaBom/PEMA">Code</a></p>
</li>
<li>
<p><strong>A Transformer-based Function Symbol Name Inference Model from an Assembly Language for Binary Reversing</strong><br />
<strong>HyunJin Kim</strong>, JinYeong Bak, Kyunghyun Cho, and Hyungjoon Koo<br />
  [In the 18th ACM Asia Conference on Computer and Communications Security (ASIACCS)] 2023.<br />
<a href="https://dl.acm.org/doi/abs/10.1145/3579856.3582823">PDF</a>
  <a href="https://github.com/agwaBom/AsmDepictor">Code</a></p>
</li>
<li>
<p><strong>Associative Knowledge Graph Using Fuzzy Clustering and Min-Max Normalization in Video Contents</strong><br />
<strong>Hyun-Jin Kim</strong>, Ji-Won Baek, Kyungyong Chung<br />
  [IEEE Access] 2021.<br />
<a href="https://ieeexplore.ieee.org/document/9430567">Link</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9430567">PDF</a>  </p>
</li>
</ol>
<h2>Domestic Conferences and Journals</h2>
<ol>
<li>
<p><strong>Function Name Prediction using Binary Code with Transformer</strong><br />
<strong>HyunJin Kim</strong> and JinYeong Bak<br />
  [The Korean Institute of Information Scientists and Engineers] 2021.</p>
</li>
<li>
<p><strong>Traffic Knowledge Graph using Associative Document Weight</strong><br />
<strong>Hyun-Jin Kim</strong>, Min-Jeong Kim, Ju-Chang Kim, Kyungyong Chung<br />
  [International Conference on Convergence Technology] 2020.</p>
</li>
<li>
<p><strong>Association Rule based Video Knowledge Extraction using Object Detection Algorithm</strong><br />
<strong>Hyun-Jin Kim</strong>, Hye-Jeong Kwon, Ji-Hye Gwon, Kyungyong Chung
  [Korean Society for Internet Information] 2020.  </p>
</li>
<li>
<p><strong>Data Bias Optimization based Association Reasoning Model for Road Risk Detection.</strong><br />
  Seong-Eun Ryu, <strong>Hyun-Jin Kim</strong>, Byung-Kook Koo, Hye-Jeong Kwon, Roy C Park, Kyungyong Chung<br />
  [Journal of the Korea Convergence Society] 2020.  </p>
</li>
</ol>
            
            <h2>Award</h2>
<ul>
<li>
<p><strong>Stars of Tomorrow Internship Award of Excellence</strong>, Microsoft Research (2025)</p>
</li>
<li>
<p><strong>PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models</strong>, (1st International NLP Workshop @ KAIST 2024)<br />
  Best Poster Award</p>
</li>
</ul>
            
            <h2>Teaching Experience</h2>
<ul>
<li>
<p><strong>Open Source Software Practice</strong>, (SKKU)<br />
  Teaching Assistant {Spring 2023, Fall 2023, Spring 2024}</p>
</li>
<li>
<p><strong>K-mooc : Mathematics for AI</strong>, (SKKU)<br />
  Dev Teaching Assistant (Summer 2021)</p>
</li>
</ul>
<h2>Talks</h2>
<ul>
<li>
<p><strong>Inverse Scaling to Superalignment: Ensuring Advanced AI Systems Align with Human Values</strong><br />
  Microsoft Research Asia, (Spring 2025)</p>
</li>
<li>
<p><strong>PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models</strong><br />
  RIKEN, (Summer 2024)</p>
</li>
<li>
<p><strong>An Offsite-Tunable Plug-in External Memory Adaptation and Embedding Temporal Awareness for Retrieval-Augmented Generation</strong><br />
  SKT, (Spring 2024)</p>
</li>
<li>
<p><strong>A Transformer-based Function Symbol Name Inference Model from an Assembly Language for Binary Reversing</strong><br />
  IBM Research, (Fall 2023)</p>
</li>
<li>
<p><strong>A Transformer-based Function Symbol Name Inference Model from an Assembly Language for Binary Reversing</strong><br />
  SKKU AI Colloquium, (Fall 2022)</p>
</li>
<li>
<p><strong>Function Name Prediction from Binary Code with Transformer</strong><br />
  New York University, (Fall 2021)</p>
</li>
</ul>
            
            <h2>Work Experience</h2>
<ul>
<li>
<p>Research Intern, <strong>Microsoft Research Asia</strong>, (Fall, 2024 ~ Present)<br />
Currently working on research projects focused on superalignment and AI safety.</p>
</li>
<li>
<p>Research Intern, <strong>Deargen Inc</strong>, (Summer, 2022)<br />
  I interned with Dr. Bonggun Shin in Deargen USA. I analyzed the differences in synthetic essentiality (SE) scores by inserting new fingerprint data into a cancer dependency prediction model that contains cancer cell lines (CCLs) with different mutation environments.</p>
</li>
</ul>
            
            <h2>Academic Services</h2>
<ul>
<li>
<p><strong>NAACL 2025 Workshop WNUT</strong><br />
  Reviewer</p>
</li>
<li>
<p><strong>IJCAI 2024</strong><br />
  Student Vounteer</p>
</li>
<li>
<p><strong>NAACL 2024</strong><br />
  Student Vounteer</p>
</li>
<li>
<p><strong>ACM FAccT 2022</strong><br />
  Student Vounteer</p>
</li>
</ul>
            
            <h2>Extracurricular Activities</h2>
<ul>
<li>
<p><strong>Optimistic, Pessimistic and Realistic of Large Language Models</strong><br />
  KOFST, Assistant, 2023</p>
</li>
<li>
<p><strong>State, Limitations, and Future of Large Language Models</strong><br />
  KOFST, Assistant, 2022</p>
</li>
</ul>
            
            <h2>References</h2>
<ul>
<li><strong>Prof. JinYeong, Bak</strong>, SKKU, jy.bak@skku.edu  </li>
<li><strong>Dr. Xiaoyuan Yi</strong>, Microsoft Research Asia, xiaoyuanyi@microsoft.com  </li>
<li><strong>Dr. Young Jin, Kim</strong>, Microsoft, youki@microsoft.com  </li>
<li><strong>Dr. Bonggun, Shin</strong>, Deargen-USA, bonggun.shin@deargen.me  </li>
<li><strong>Prof. Hyungjoon Koo</strong>, SKKU, kevin.koo@skku.edu  </li>
<li><strong>Prof. Kyungyong, Chung</strong>, KGU, dragonhci@daum.net  </li>
</ul>
            
        </section>
    </div>
</body>

</html>